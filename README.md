# ğŸ§  Adversarial Image Attacks with FGSM and PGD

This project demonstrates how to generate adversarial examples using Fast Gradient Sign Method (FGSM) and Projected Gradient Descent (PGD) to fool pretrained image classification models (e.g., ResNet18). The goal is to perturb input images so that a neural network misclassifies them into a specified target class, while the perturbation remains imperceptible to the human eye.

## ğŸ“¸ Example

<p align="center">
  <img src="results/pgd_algorithm_result_overview.png" width="600" />
</p>

An image of a â€œMaltese dogâ€ is perturbed to be classified as a â€œgoldfishâ€ by the model.

---

## ğŸ“¦ Features

- ğŸ” Support for FGSM and PGD attacks
- ğŸ–¼ï¸ Visualizations of original image, perturbation, and adversarial result
- ğŸ“Š Confidence scores for original and adversarial predictions
- ğŸ§ª Jupyter notebook demo and CLI interface
- ğŸ§  Works with any torchvision-compatible model (e.g., ResNet, ViT)

---

## ğŸ“ Project Structure

```
adversarial_noise_code_challenge/
â”œâ”€â”€ adversarial_attack_lib/
â”‚   â”œâ”€â”€ __init__.py           
â”‚   â”œâ”€â”€ attack.py             # Algorithms to generate adversarial images
â”‚   â”œâ”€â”€ model.py              # Load models and preprocess image data helper functions
â”‚   â”œâ”€â”€ runner.py             # Helper function to run adversarial image generation function 
â”‚   â”œâ”€â”€ utils.py              # Helper functions (get prediction confidence and visualize outputs)
â”‚
â”‚   main.py                   # CLI main entry point
â”œâ”€â”€ examples/
â”‚   â””â”€â”€ original_images/      # Sample input images
â”‚   â””â”€â”€ AdversarialDemo.ipynb # Interactive demo with visualizations
â”œâ”€â”€ results/                  # Output images (original, adversarial)
|
â”œâ”€â”€ README.md
â””â”€â”€ requirements.txt
```

---

## ğŸš€ Getting Started

### 1. Install Requirements

```bash
pip install -r requirements.txt
```

### 2. Run from Python Script

```bash
python main.py \
  --image examples/original_images/dog.JPEG \
  --target_class goldfish \
  --attack fgsm \
  --epsilon 0.03 \
  --model resnet18
```

### 3. Run from Jupyter Notebook

Use `examples/AdversarialDemo.ipynb` to experiment interactively.

---

## âœï¸ Example Usage (Python)

```python
from adversarial_attack_lib.runner import run_attack

result = run_attack(
    image_path="examples/original_images/dog.JPEG",
    target_class="goldfish",
    attack_type="pgd",
    epsilon=0.03,
    alpha=0.005,
    steps=10,
    model_name="resnet18",
    save_output=True
)

# Visualize result
from adversarial_attack_lib.utils import visualize_attack
visualize_attack(result)
```

---

## âœ… Requirements

- Python 3.8+
- PyTorch
- torchvision
- matplotlib
- Pillow

---

## ğŸ§  Author

Created by John E. Fleming 