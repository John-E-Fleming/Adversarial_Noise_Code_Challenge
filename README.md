# ðŸ§  Adversarial Image Attacks

This project demonstrates how to generate adversarial noise to fool pretrained image classification models (e.g., ResNet18) to misclassify an images as a specified target class. The goal is to perturb input images with the generated adversial noise so that a neural network misclassifies them into a specified target class, while the perturbation remains imperceptible to the human eye.

## ðŸ“¸ Example Result

<p align="center">
  <img src="results/pgd_algorithm_result_overview.png" width="600" />
</p>

An image of a "spider monkeyâ€ is perturbed to be classified as a â€œgoldfishâ€ with high confidence by the model. The Projected Gradient Descent (PGD) algorithm was used to generate the presented image. 

---

## ðŸ“¦ Features

- ðŸ” Support two adversial noise generation algorithms Fast Gradient Sign Method (FGSM) and Project Gradient Descent (PGD) attacks
- ðŸ–¼ï¸ Visualizations of original image, perturbation, and adversarial result
- ðŸ“Š Confidence scores for original and adversarial predictions
- ðŸ§ª Jupyter notebook demo and CLI interface
- ðŸ§  Works with any torchvision-compatible model (e.g., ResNet, ViT)

---

## ðŸ“ Project Structure

```
adversarial_noise_code_challenge/
â”œâ”€â”€ adversarial_attack_lib/
â”‚   â”œâ”€â”€ __init__.py           
â”‚   â”œâ”€â”€ attack.py             # Algorithms to generate adversarial images
â”‚   â”œâ”€â”€ model.py              # Load models and preprocess image data helper functions
â”‚   â”œâ”€â”€ runner.py             # Helper function to run adversarial image generation function 
â”‚   â””â”€â”€ utils.py              # Helper functions (get prediction confidence and visualize outputs)
â”‚
â”œâ”€â”€ main.py                   # CLI main entry point
â”œâ”€â”€ examples/
â”‚   â””â”€â”€ original_images/      # Sample input images
â”‚   â””â”€â”€ AdversarialDemo.ipynb # Interactive demo with visualizations
â”œâ”€â”€ results/                  # Output images (original, adversarial)
|
â”œâ”€â”€ README.md
â””â”€â”€ requirements.txt
```

---

## ðŸš€ Getting Started

### 1. Install Requirements

```bash
pip install -r requirements.txt
```

### 2. Run Python Script from the Command Line

```bash
python main.py \
  --image examples/original_images/monkey.JPEG \
  --target_class goldfish \
  --attack pgd \
  --epsilon 0.03 \
  --model resnet18 \
  --alpha 0.005 \
  --steps 10
```

### 3. Run from Jupyter Notebook

Use `examples/AdversarialDemo.ipynb` to experiment interactively.

---

## âœï¸ Example Usage (Python)

```python
from adversarial_attack_lib.runner import run_attack

result = run_attack(
    image_path="examples/original_images/dog.JPEG",
    target_class="goldfish",
    attack_type="pgd",
    epsilon=0.03,
    alpha=0.005,
    steps=10,
    model_name="resnet18",
    save_output=True
)

# Visualize result
from adversarial_attack_lib.utils import visualize_attack
visualize_attack(result)
```

---

## âœ… Requirements

- Python 3.8+
- PyTorch
- torchvision
- matplotlib
- Pillow

---

## âœ… To Do

 - [ ] Add additional early stopping conditions for the PGD algorithm implementation (i.e. stop when prediction confidence in target class is above a specified threshold)
 - [ ] Update attack methods to support untargeted attacks also (i.e. just minimize the confidence in the true class label)
 - [ ] Add evaluation of k-top class predictions (i.e. what classes are increase in prediction confidence following addition of the adversarial noise)
 - [ ] Include a methods to counteract the affect of or detect images with adversarial noise (i.e. detect adversial images by using a auxillary classifier to detect perturbations -> input gradient norms could be used as an anomaly signal)

---

## ðŸ§  Author

Created by John E. Fleming 